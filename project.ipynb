{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing with Disaster Tweets\n",
    "\n",
    "In this project, we will be using a dataset of disaster tweets. The goal is to classify whether a tweet is about a real disaster or not.\n",
    "\n",
    "We will be using the following techniques:\n",
    "\n",
    "1. Data Cleaning\n",
    "2. Data Exploration\n",
    "3. Data Visualization\n",
    "4. Feature Extraction\n",
    "5. Model Building   \n",
    "\n",
    "\n",
    "Assumptions: The words mentioned in the tweet are a good indication of the tweet's sentiment. If we can find a pattern in the words mentioned in the tweet, we can classify the tweet as a disaster tweet or not.\n",
    "Words that are repeated in the tweet are a good indication of the tweet's sentiment.\n",
    "\n",
    "Hypothesis: If a tweet contains a high amount of similar words, it is more likely to be a disaster tweet.\n",
    "\n",
    "Model choice: Due to the linear relationship between the words in the tweet and the sentiment of the tweet, we will be using a linear model. We will start with ridge regression and then try other models if the performance is not good.\n",
    "\n",
    "Approach: The first step is to obtain a score for the words in the tweet based on the kaggle exmaple given to us. We will thengradually improve the complexity of the test at different stages to see how the score improves.\n",
    "\n",
    "Kaggle example approach: \n",
    "    In the example, they chose to use a ridge regression model. As a result, the data, which was already split into test and training data, was transformed into a training df with the tweet number and the frequency of similar words in the tweet using count vectorization. \n",
    "    The count vectorization found the frequency of similar words in all the tweets and created a vector for each tweet containing the frequency of the similar words in the tweet. \n",
    "    This training df was then used to train a ridge regression model. This was done using cross validation where the scoring method was the F1 score. The training df was divided into three parts, training the model on two parts while testing on the third, and repeating this process three times with different testing parts. This results in an array of three F1 scores, one for each fold.\n",
    "    Since the average score was decent, the model was then fit to the entire training df (the train_vectors) with the training data target column.\n",
    "    Lastly, the fitted model was used to predict the sentiment of the testing data.\n",
    "\n",
    "\n",
    "\n",
    "The model was then used to predict the sentiment of the testing data using cross validation where the \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "What's up man?\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('/opt/miniconda3/envs/NLP_distater_tweets/nlp-getting-started/test.csv')\n",
    "df_train = pd.read_csv('/opt/miniconda3/envs/NLP_distater_tweets/nlp-getting-started/train.csv')\n",
    "\n",
    "\n",
    "print(df_train[df_train[\"target\"] == 1][\"text\"].values[0])\n",
    "print(df_train[df_train[\"target\"] == 0][\"text\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 54)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "## let's get counts for the first 5 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(df_train[\"text\"][0:5])\n",
    "#print(example_train_vectors)\n",
    "print(example_train_vectors.todense().shape)\n",
    "#print(example_train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## let's get counts for the first 5 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(df_train[\"text\"][0:5])\n",
    "#print(example_train_vectors)\n",
    "print(example_train_vectors[0].todense().shape)\n",
    "print(example_train_vectors[0].todense())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_distater_tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
